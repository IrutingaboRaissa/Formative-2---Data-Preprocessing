{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4580fd",
   "metadata": {},
   "source": [
    "# Part 2: Facial Recognition System\n",
    "## Formative 2 - Data Preprocessing Assignment\n",
    "\n",
    "This notebook implements facial recognition for user authentication.\n",
    "\n",
    "### Tasks:\n",
    "1. Load and display facial images (neutral, smiling, surprised)\n",
    "2. Apply image augmentations (rotation, flipping, grayscale, etc.)\n",
    "3. Extract image features (embeddings, histograms)\n",
    "4. Train facial recognition model\n",
    "5. Evaluate model performance (Accuracy, F1-Score)\n",
    "6. Demonstrate authentication system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547c597",
   "metadata": {},
   "source": [
    "## 1. Image Data Collection and Display\n",
    "\n",
    "Each group member should provide 3 images:\n",
    "- Neutral expression\n",
    "- Smiling expression\n",
    "- Surprised expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory structure\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, 'images')\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "FEATURE_DIR = os.path.join(BASE_DIR, 'features')\n",
    "\n",
    "# Create subdirectories for each member and expression\n",
    "# Structure: images/member_name/expression/image.jpg\n",
    "members = ['member1', 'member2', 'member3']  # Replace with actual names\n",
    "expressions = ['neutral', 'smiling', 'surprised']\n",
    "\n",
    "for member in members:\n",
    "    for expression in expressions:\n",
    "        path = os.path.join(IMAGE_DIR, member, expression)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure created!\")\n",
    "print(f\"Please place images in: {IMAGE_DIR}\")\n",
    "print(\"Format: images/member_name/expression/image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_dir):\n",
    "    \"\"\"\n",
    "    Load all images from the directory structure.\n",
    "    Returns: DataFrame with image paths, labels, and pixel data\n",
    "    \"\"\"\n",
    "    images_data = []\n",
    "    \n",
    "    for member in os.listdir(image_dir):\n",
    "        member_path = os.path.join(image_dir, member)\n",
    "        if not os.path.isdir(member_path):\n",
    "            continue\n",
    "            \n",
    "        for expression in os.listdir(member_path):\n",
    "            expression_path = os.path.join(member_path, expression)\n",
    "            if not os.path.isdir(expression_path):\n",
    "                continue\n",
    "                \n",
    "            for img_file in os.listdir(expression_path):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(expression_path, img_file)\n",
    "                    \n",
    "                    # Load image\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                        images_data.append({\n",
    "                            'path': img_path,\n",
    "                            'member': member,\n",
    "                            'expression': expression,\n",
    "                            'image': img_rgb,\n",
    "                            'filename': img_file\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(images_data)\n",
    "\n",
    "# Load images\n",
    "df_images = load_images(IMAGE_DIR)\n",
    "print(f\"Loaded {len(df_images)} images\")\n",
    "print(f\"\\nMembers: {df_images['member'].unique()}\")\n",
    "print(f\"Expressions: {df_images['expression'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(df, num_samples=3):\n",
    "    \"\"\"\n",
    "    Display sample images for each member and expression\n",
    "    \"\"\"\n",
    "    members = df['member'].unique()\n",
    "    expressions = df['expression'].unique()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(members), len(expressions), \n",
    "                             figsize=(15, 5*len(members)))\n",
    "    \n",
    "    if len(members) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, member in enumerate(members):\n",
    "        for j, expression in enumerate(expressions):\n",
    "            subset = df[(df['member'] == member) & (df['expression'] == expression)]\n",
    "            \n",
    "            if len(subset) > 0:\n",
    "                img = subset.iloc[0]['image']\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].set_title(f\"{member} - {expression}\")\n",
    "                axes[i, j].axis('off')\n",
    "            else:\n",
    "                axes[i, j].text(0.5, 0.5, 'No Image', \n",
    "                               ha='center', va='center')\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images\n",
    "if len(df_images) > 0:\n",
    "    display_sample_images(df_images)\n",
    "else:\n",
    "    print(\"No images found. Please add images to the 'images' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d4a27",
   "metadata": {},
   "source": [
    "## 2. Image Augmentation\n",
    "\n",
    "Apply at least 2 augmentations per image:\n",
    "- Rotation\n",
    "- Horizontal flip\n",
    "- Grayscale conversion\n",
    "- Brightness adjustment\n",
    "- Gaussian blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    \"\"\"\n",
    "    Apply multiple augmentations to an image.\n",
    "    Returns: List of augmented images with labels\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    \n",
    "    # Original\n",
    "    augmented.append(('original', img))\n",
    "    \n",
    "    # Rotation (15 degrees)\n",
    "    height, width = img.shape[:2]\n",
    "    center = (width // 2, height // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, 15, 1.0)\n",
    "    rotated = cv2.warpAffine(img, rotation_matrix, (width, height))\n",
    "    augmented.append(('rotated_15', rotated))\n",
    "    \n",
    "    # Rotation (-15 degrees)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, -15, 1.0)\n",
    "    rotated_neg = cv2.warpAffine(img, rotation_matrix, (width, height))\n",
    "    augmented.append(('rotated_-15', rotated_neg))\n",
    "    \n",
    "    # Horizontal flip\n",
    "    flipped = cv2.flip(img, 1)\n",
    "    augmented.append(('flipped', flipped))\n",
    "    \n",
    "    # Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gray_rgb = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)\n",
    "    augmented.append(('grayscale', gray_rgb))\n",
    "    \n",
    "    # Brightness increase\n",
    "    bright = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n",
    "    augmented.append(('brightness_up', bright))\n",
    "    \n",
    "    # Brightness decrease\n",
    "    dark = cv2.convertScaleAbs(img, alpha=0.8, beta=-30)\n",
    "    augmented.append(('brightness_down', dark))\n",
    "    \n",
    "    # Gaussian blur\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    augmented.append(('blurred', blurred))\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Apply augmentation to all images\n",
    "augmented_data = []\n",
    "\n",
    "for idx, row in df_images.iterrows():\n",
    "    augmentations = augment_image(row['image'])\n",
    "    \n",
    "    for aug_type, aug_img in augmentations:\n",
    "        augmented_data.append({\n",
    "            'member': row['member'],\n",
    "            'expression': row['expression'],\n",
    "            'augmentation': aug_type,\n",
    "            'image': aug_img,\n",
    "            'original_path': row['path']\n",
    "        })\n",
    "\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "print(f\"Total augmented images: {len(df_augmented)}\")\n",
    "print(f\"Original images: {len(df_images)}\")\n",
    "print(f\"Augmentations per image: {len(df_augmented) // len(df_images) if len(df_images) > 0 else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display augmentation examples\n",
    "def display_augmentations(df, member_idx=0, expression='neutral'):\n",
    "    \"\"\"\n",
    "    Display all augmentations for a single image\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"No images to display\")\n",
    "        return\n",
    "    \n",
    "    members = df['member'].unique()\n",
    "    if member_idx >= len(members):\n",
    "        member_idx = 0\n",
    "    \n",
    "    member = members[member_idx]\n",
    "    subset = df[(df['member'] == member) & (df['expression'] == expression)]\n",
    "    \n",
    "    if len(subset) == 0:\n",
    "        print(f\"No images found for {member} - {expression}\")\n",
    "        return\n",
    "    \n",
    "    augmentations = subset['augmentation'].unique()\n",
    "    n_aug = len(augmentations)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, aug_type in enumerate(augmentations[:8]):\n",
    "        img_data = subset[subset['augmentation'] == aug_type].iloc[0]\n",
    "        axes[i].imshow(img_data['image'])\n",
    "        axes[i].set_title(f\"{aug_type}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_aug, 8):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Augmentations for {member} - {expression}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display augmentations for first member\n",
    "if len(df_augmented) > 0:\n",
    "    display_augmentations(df_augmented, member_idx=0, expression='neutral')\n",
    "else:\n",
    "    print(\"No augmented images to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9eb2e9",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "Extract features from images:\n",
    "- Color histograms (RGB channels)\n",
    "- HOG (Histogram of Oriented Gradients)\n",
    "- Statistical features (mean, std, etc.)\n",
    "- Edge detection features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d68b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "\n",
    "def extract_features(img, resize_shape=(128, 128)):\n",
    "    \"\"\"\n",
    "    Extract comprehensive features from an image.\n",
    "    Returns: Feature vector as numpy array\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Resize image for consistency\n",
    "    img_resized = cv2.resize(img, resize_shape)\n",
    "    \n",
    "    # 1. Color Histogram Features (RGB)\n",
    "    for channel in range(3):\n",
    "        hist = cv2.calcHist([img_resized], [channel], None, [32], [0, 256])\n",
    "        hist = hist.flatten() / hist.sum()  # Normalize\n",
    "        features.extend(hist)\n",
    "    \n",
    "    # 2. Statistical Features\n",
    "    for channel in range(3):\n",
    "        channel_data = img_resized[:, :, channel]\n",
    "        features.append(np.mean(channel_data))\n",
    "        features.append(np.std(channel_data))\n",
    "        features.append(np.median(channel_data))\n",
    "        features.append(np.min(channel_data))\n",
    "        features.append(np.max(channel_data))\n",
    "    \n",
    "    # 3. Grayscale features\n",
    "    gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Edge detection (Canny)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    features.append(np.sum(edges > 0) / edges.size)  # Edge density\n",
    "    \n",
    "    # 4. HOG Features\n",
    "    hog_features = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                       cells_per_block=(2, 2), visualize=False)\n",
    "    # Take first 100 HOG features to reduce dimensionality\n",
    "    features.extend(hog_features[:100])\n",
    "    \n",
    "    # 5. Texture features (using Laplacian)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    features.append(np.var(laplacian))  # Texture variance\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features from all augmented images\n",
    "print(\"Extracting features from images...\")\n",
    "feature_list = []\n",
    "\n",
    "for idx, row in df_augmented.iterrows():\n",
    "    features = extract_features(row['image'])\n",
    "    \n",
    "    feature_dict = {\n",
    "        'member': row['member'],\n",
    "        'expression': row['expression'],\n",
    "        'augmentation': row['augmentation']\n",
    "    }\n",
    "    \n",
    "    # Add features as separate columns\n",
    "    for i, feat_val in enumerate(features):\n",
    "        feature_dict[f'feature_{i}'] = feat_val\n",
    "    \n",
    "    feature_list.append(feature_dict)\n",
    "\n",
    "df_features = pd.DataFrame(feature_list)\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Shape: {df_features.shape}\")\n",
    "print(f\"Features per image: {len([col for col in df_features.columns if 'feature_' in col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features to CSV\n",
    "feature_csv_path = os.path.join(FEATURE_DIR, 'image_features.csv')\n",
    "df_features.to_csv(feature_csv_path, index=False)\n",
    "print(f\"Features saved to: {feature_csv_path}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06696f4f",
   "metadata": {},
   "source": [
    "## 4. Facial Recognition Model\n",
    "\n",
    "Train a model to recognize different team members based on their facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac348c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model training\n",
    "if len(df_features) > 0:\n",
    "    # Separate features and labels\n",
    "    feature_columns = [col for col in df_features.columns if 'feature_' in col]\n",
    "    X = df_features[feature_columns].values\n",
    "    y = df_features['member'].values\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Number of members: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Members: {label_encoder.classes_}\")\n",
    "else:\n",
    "    print(\"No features available for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea693326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Model\n",
    "if len(df_features) > 0:\n",
    "    print(\"Training Random Forest Classifier...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = rf_model.predict(X_train_scaled)\n",
    "    y_pred_test = rf_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    train_f1 = f1_score(y_train, y_pred_train, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    \n",
    "    print(f\"\\n--- Random Forest Results ---\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Training F1-Score: {train_f1:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test, \n",
    "                                target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression Model\n",
    "if len(df_features) > 0:\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    lr_model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        multi_class='multinomial'\n",
    "    )\n",
    "    \n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train_lr = lr_model.predict(X_train_scaled)\n",
    "    y_pred_test_lr = lr_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_accuracy_lr = accuracy_score(y_train, y_pred_train_lr)\n",
    "    test_accuracy_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "    train_f1_lr = f1_score(y_train, y_pred_train_lr, average='weighted')\n",
    "    test_f1_lr = f1_score(y_test, y_pred_test_lr, average='weighted')\n",
    "    \n",
    "    print(f\"\\n--- Logistic Regression Results ---\")\n",
    "    print(f\"Training Accuracy: {train_accuracy_lr:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "    print(f\"Training F1-Score: {train_f1_lr:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1_lr:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test_lr, \n",
    "                                target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "if len(df_features) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Random Forest Confusion Matrix\n",
    "    cm_rf = confusion_matrix(y_test, y_pred_test)\n",
    "    sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_,\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title('Random Forest - Confusion Matrix')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Logistic Regression Confusion Matrix\n",
    "    cm_lr = confusion_matrix(y_test, y_pred_test_lr)\n",
    "    sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_,\n",
    "                ax=axes[1])\n",
    "    axes[1].set_title('Logistic Regression - Confusion Matrix')\n",
    "    axes[1].set_ylabel('True Label')\n",
    "    axes[1].set_xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f395866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "if len(df_features) > 0:\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': ['Random Forest', 'Logistic Regression'],\n",
    "        'Train Accuracy': [train_accuracy, train_accuracy_lr],\n",
    "        'Test Accuracy': [test_accuracy, test_accuracy_lr],\n",
    "        'Train F1-Score': [train_f1, train_f1_lr],\n",
    "        'Test F1-Score': [test_f1, test_f1_lr]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== Model Performance Comparison ===\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    comparison_df.plot(x='Model', y=['Train Accuracy', 'Test Accuracy'], \n",
    "                       kind='bar', ax=axes[0], rot=0)\n",
    "    axes[0].set_title('Accuracy Comparison')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_ylim([0, 1.1])\n",
    "    axes[0].legend(['Train Accuracy', 'Test Accuracy'])\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    comparison_df.plot(x='Model', y=['Train F1-Score', 'Test F1-Score'],\n",
    "                       kind='bar', ax=axes[1], rot=0, color=['green', 'orange'])\n",
    "    axes[1].set_title('F1-Score Comparison')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_ylim([0, 1.1])\n",
    "    axes[1].legend(['Train F1-Score', 'Test F1-Score'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36256261",
   "metadata": {},
   "source": [
    "## 5. Save Models and Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11462409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessing objects\n",
    "if len(df_features) > 0:\n",
    "    # Save Random Forest model\n",
    "    rf_model_path = os.path.join(MODEL_DIR, 'face_recognition_rf.pkl')\n",
    "    joblib.dump(rf_model, rf_model_path)\n",
    "    print(f\"Random Forest model saved to: {rf_model_path}\")\n",
    "    \n",
    "    # Save Logistic Regression model\n",
    "    lr_model_path = os.path.join(MODEL_DIR, 'face_recognition_lr.pkl')\n",
    "    joblib.dump(lr_model, lr_model_path)\n",
    "    print(f\"Logistic Regression model saved to: {lr_model_path}\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = os.path.join(MODEL_DIR, 'scaler.pkl')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved to: {scaler_path}\")\n",
    "    \n",
    "    # Save label encoder\n",
    "    encoder_path = os.path.join(MODEL_DIR, 'label_encoder.pkl')\n",
    "    joblib.dump(label_encoder, encoder_path)\n",
    "    print(f\"Label encoder saved to: {encoder_path}\")\n",
    "    \n",
    "    print(\"\\n✓ All models and preprocessing objects saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027de1f5",
   "metadata": {},
   "source": [
    "## 6. Facial Recognition Demo\n",
    "\n",
    "Test the facial recognition system with new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(image_path, model, scaler, label_encoder, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Recognize a face from an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: Trained classifier\n",
    "        scaler: Feature scaler\n",
    "        label_encoder: Label encoder\n",
    "        threshold: Confidence threshold for recognition\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with recognition results\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return {'recognized': False, 'error': 'Could not load image'}\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(img_rgb)\n",
    "    features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    probabilities = model.predict_proba(features_scaled)[0]\n",
    "    confidence = probabilities.max()\n",
    "    \n",
    "    # Check confidence threshold\n",
    "    if confidence >= threshold:\n",
    "        member_name = label_encoder.inverse_transform([prediction])[0]\n",
    "        return {\n",
    "            'recognized': True,\n",
    "            'member': member_name,\n",
    "            'confidence': confidence,\n",
    "            'all_probabilities': dict(zip(label_encoder.classes_, probabilities))\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'recognized': False,\n",
    "            'reason': 'Low confidence',\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "# Test with a sample image from test set\n",
    "if len(df_features) > 0 and len(df_images) > 0:\n",
    "    test_image_path = df_images.iloc[0]['path']\n",
    "    \n",
    "    print(\"Testing facial recognition...\")\n",
    "    result = recognize_face(test_image_path, rf_model, scaler, label_encoder)\n",
    "    \n",
    "    # Display result\n",
    "    img = cv2.imread(test_image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img_rgb)\n",
    "    \n",
    "    if result['recognized']:\n",
    "        title = f\"✓ Recognized: {result['member']}\\nConfidence: {result['confidence']:.2%}\"\n",
    "        plt.title(title, color='green', fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        title = f\"✗ Not Recognized\\n{result.get('reason', 'Unknown')}\"\n",
    "        plt.title(title, color='red', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nRecognition Result:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d76c0a",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (Random Forest)\n",
    "if len(df_features) > 0:\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    \n",
    "    # Get top 20 most important features\n",
    "    top_n = 20\n",
    "    indices = np.argsort(feature_importance)[-top_n:]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(top_n), feature_importance[indices])\n",
    "    plt.yticks(range(top_n), [f'Feature {i}' for i in indices])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {top_n} Most Important Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7dccd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Completed Tasks:\n",
    "1. ✓ Image data collection and display (3 expressions per member)\n",
    "2. ✓ Image augmentation (8 augmentations per image)\n",
    "3. ✓ Feature extraction (histograms, HOG, statistical features)\n",
    "4. ✓ Saved features to `image_features.csv`\n",
    "5. ✓ Trained facial recognition models (Random Forest & Logistic Regression)\n",
    "6. ✓ Model evaluation (Accuracy, F1-Score)\n",
    "7. ✓ Saved trained models for deployment\n",
    "8. ✓ Created recognition demo function\n",
    "\n",
    "### Next Steps:\n",
    "- Add your team members' facial images to the `images` folder\n",
    "- Re-run the notebook to train on actual data\n",
    "- Integrate with voice verification and product recommendation models\n",
    "- Create command-line demo application"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
